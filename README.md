# 2025年度 B3向け 教師なしSimCSEの実装

本リポジトリには、B3向けの自然言語処理に関する研究を体験してもらうためのコードを公開しています。

## データの前処理
訓練に使用するデータとして、今回はWikipediaコーパスを使用します。
[SimCSE](https://aclanthology.org/2021.emnlp-main.552/)の論文内で公開されているWikipediaをダウンロードしています。
また、前処理として、10単語以上50単語以下からなるテキストデータのみを今回は使用します。

## 対照学習
tokenizeの設定、SimCSEのモデル、lossの設定等を行っています。
GoogleColab上で簡単に実行するため、訓練用データ数は20,000件、検証用データ数は2,000件です。
データ数は各自で設定しください。

## 評価
モデルの評価には、STS-Bの評価用データを用いてspearman係数を測定します。
モデルの[CLS]トークンの埋め込みを文埋め込みとして、2文間の類似度を計算します。


